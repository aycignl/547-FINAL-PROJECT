{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Active Selection of Elements with Variational Bayes for Nonnegative Matrix Factorization\n",
    "\n",
    "<br>\n",
    "<center>  Abdullatif Köksal, Burak Suyunu, Gönül Aycı, Mustafa Melih Mutlu, A.Taylan Cemgil </center>\n",
    "<center> * Bilgisayar Mühendisliği Bölümü, Boğaziçi Üniversitesi </center>\n",
    "<center> * {abdullatif.koksal, burak.suyunu, gonul.ayci, melih.mutlu, taylan.cemgil}@boun.edu.tr </center>\n",
    "\n",
    "## Özetçe\n",
    "\n",
    "Klasik matris tamamlama problemlerinde elimizdeki matris, gözlemlenen ve bilinmeyen elemanlar olarak iki gruba ayrılabilir. Bu çalışmadaki yaklaşımda ise matrisler üç farklı gruptan oluşmaktadır: bilinen ve masrafsız olarak her zaman erişebildigimiz gözlemlenmiş olan veri, tahmin etmeye çalıştığımız bilinmeyen veri ve şu an bilinmeyen ancak istenildigi zaman sorgulanabilen veri. Son gruptaki veriler ilk kez sorgulandığında bir maliyet ortaya çıkmaktadır. Bu gözlemden yola çıkarak, mümkün olduğu kadar az sorgu yaparak ikinci gruptaki bilinmeyen verideki değerleri en az hata ile tahminlemek istiyoruz. Amacımız, sorgulamaya çalıştığımız gözlemleri akıllıca seçebilmek. Bu çalışmamızda, gözlem sırası seçme stratejileri tanımlanarak MovieLens veri setinde karşılaştırılmıştır.\n",
    "\n",
    "## Abstract\n",
    "In classical matrix completion problems, you can divide the matrix into two groups as observed and unknown elements. In this study, the matrices are composed of three different groups: observed data that is known and accessible at any time without any expense, unknown data that we are trying to predict, and data that is currently unknown but can be queried when desired. When the data in the last group is queried for the first time, a cost arises. From this observation, we want to estimate the unknown data in the second group with the least error by making as few queries as possible. Our goal is to choose the observations we are trying to question wisely. In this study, observation sequence selections were defined and compared in\n",
    "the MovieLens data set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Giriş\n",
    "Negatif olmayan matris ayrıştırma yöntemi (NOMA), ilk olarak Lee ve Seung [1] tarafından önerilmiştir. Negatif olmayan matris ayrıştırması, verilen negatif olmayan bir *X* matrisinin negatif olamayan değerler içeren *T* ve *V* çarpanlarına ayırma yöntemidir. Elde edilen iki matrisin çarpımlarının değeri ayrıştırılan matrisin değerine yaklaşık olarak eşittir.\n",
    "\n",
    "Öneri sistemlerinde ayrışım tabanlı yapılar sıklıkla kullanılmaktadır. Sistemin mantığı şu şekilde açıklanabilir. Bir *X* matrisi ile belirli bir süre aralıgında toplanmış olan kullanıcı-film ilişkisine dair verimizi gösterelim. Bu matrisin satırları filmleri, sütunları kullanıcıları, elemanları ise kullanıcıların filmlere vermiş oldugu puanlamaları temsil etmektedir. Eğer *X* matrisinin herhangi bir elemanının değeri yok ise, bu kullanıcı ve film arasında henüz bir ilişkinin olmadığını göstermektedir yani kullanıcı bu filme henüz herhangi bir oy vermemiştir. Kullanıcılara etkileşimli olarak film hakkındaki puanlamaları sorulabilir. Ancak herkesin her zaman her film hakkında puanlama yapması mümkün degildir. Dolayısıyla bir kişiye bir film hakkındaki puanını sorarken akıllıca bir yol izlemek gerekmektedir. Bu şekilde en az kişiden bilgi talep ederek, elde edilen bilgilerle kullanıcı-film arasındaki örüntüye ulaşmak ve bilmedigimiz veriler hakkındaki tahminlerimizi iyileştirmek istenmektedir.\n",
    "\n",
    "<img src=\"senaryo.png\" width=\"400px\">\n",
    "<figcaption>Şekil 1: **Senaryonun işlenişi:** Burada mavi hücreler maskelenmiş, kırmızılar test ve beyazlar ise gözlemlenmiş veriyi göstermektedir. Kırmızının koyuluğu hatanın fazlalığına işaret etmektedir. Bir veri gözlemlendiginde en çok bulunduğu satır ve sütun hakkında bilgi vermesi beklenmektedir. *Senaryo B*’nin test hücreleri hakkında verdiği bilgi *Senaryo A*’dan fazla olduğu görülmektedir.</figcaption>\n",
    "\n",
    "\n",
    "**Senaryo:** Elimizde bir *X* matrisimiz olsun. Verilen *X* matrisine Varyasyonel Bayes ile negatif olmayan matris ayrıştırması yöntemi kullanarak yaklaşım yapıyoruz. Matrisimizi\n",
    "bildiğimiz, bilmediğimiz (tahmin etmeye çalıştığımız) ve zaman içinde açarak gözlemleyeceğimiz (maskelenmiş) üç çeşit veriden oluşturuyoruz. Üçüncü kategorideki veriden belli yığınlarda ve minimum sayıda veri açarak maksimum bilgi edinmeyi ve en iyi tahminleme yapmayı hedefliyoruz. Bu gruptaki veriyi nasıl seçecegimiz konusunda tanımladığımız çeşitli gözlem sırası seçme stratejilerimizi karşılaştırıyoruz."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy as sp\n",
    "import math\n",
    "import time\n",
    "from scipy import special\n",
    "from scipy.stats import gamma\n",
    "from scipy.stats import entropy\n",
    "from scipy.integrate import simps\n",
    "from scipy.special import digamma\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "from sklearn import preprocessing\n",
    "import random\n",
    "import pandas as pd\n",
    "from ipywidgets import *\n",
    "import operator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MovieLens Veri Seti\n",
    "\n",
    "<cite>[MovieLens][1]</cite> veri setinde 700 kullanıcının 9000 filme vermiş olduğu 0,5 ile 5 puan arasında degişen toplam 100.000 oy bulunmaktadır. Modelimizdeki çok terimli dagılıma uygun bir girdi oluşturmak için oy puan aralıgını 0,5-5 ten 1-10 aralığına eşledik. \n",
    "\n",
    "[1]:https://grouplens.org/datasets/movielens/latest/\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_MovieLens = pd.read_csv('ratings.csv')\n",
    "#df_MovieLens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100004"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df_MovieLens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>userId</th>\n",
       "      <th>movieId</th>\n",
       "      <th>rating</th>\n",
       "      <th>timestamp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>31</td>\n",
       "      <td>2.5</td>\n",
       "      <td>1260759144</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1029</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1260759179</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>1061</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1260759182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1129</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1260759185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>1172</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1260759205</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1</td>\n",
       "      <td>1263</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1260759151</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1</td>\n",
       "      <td>1287</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1260759187</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1</td>\n",
       "      <td>1293</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1260759148</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1</td>\n",
       "      <td>1339</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1260759125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1</td>\n",
       "      <td>1343</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1260759131</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1</td>\n",
       "      <td>1371</td>\n",
       "      <td>2.5</td>\n",
       "      <td>1260759135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1</td>\n",
       "      <td>1405</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1260759203</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1</td>\n",
       "      <td>1953</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1260759191</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>1</td>\n",
       "      <td>2105</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1260759139</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>1</td>\n",
       "      <td>2150</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1260759194</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>1</td>\n",
       "      <td>2193</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1260759198</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>1</td>\n",
       "      <td>2294</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1260759108</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>1</td>\n",
       "      <td>2455</td>\n",
       "      <td>2.5</td>\n",
       "      <td>1260759113</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>1</td>\n",
       "      <td>2968</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1260759200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>1</td>\n",
       "      <td>3671</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1260759117</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>2</td>\n",
       "      <td>10</td>\n",
       "      <td>4.0</td>\n",
       "      <td>835355493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>2</td>\n",
       "      <td>17</td>\n",
       "      <td>5.0</td>\n",
       "      <td>835355681</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>2</td>\n",
       "      <td>39</td>\n",
       "      <td>5.0</td>\n",
       "      <td>835355604</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>2</td>\n",
       "      <td>47</td>\n",
       "      <td>4.0</td>\n",
       "      <td>835355552</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>2</td>\n",
       "      <td>50</td>\n",
       "      <td>4.0</td>\n",
       "      <td>835355586</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>2</td>\n",
       "      <td>52</td>\n",
       "      <td>3.0</td>\n",
       "      <td>835356031</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>2</td>\n",
       "      <td>62</td>\n",
       "      <td>3.0</td>\n",
       "      <td>835355749</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>2</td>\n",
       "      <td>110</td>\n",
       "      <td>4.0</td>\n",
       "      <td>835355532</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>2</td>\n",
       "      <td>144</td>\n",
       "      <td>3.0</td>\n",
       "      <td>835356016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>2</td>\n",
       "      <td>150</td>\n",
       "      <td>5.0</td>\n",
       "      <td>835355395</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99974</th>\n",
       "      <td>671</td>\n",
       "      <td>4034</td>\n",
       "      <td>4.5</td>\n",
       "      <td>1064245493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99975</th>\n",
       "      <td>671</td>\n",
       "      <td>4306</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1064245548</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99976</th>\n",
       "      <td>671</td>\n",
       "      <td>4308</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1065111985</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99977</th>\n",
       "      <td>671</td>\n",
       "      <td>4880</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1065111973</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99978</th>\n",
       "      <td>671</td>\n",
       "      <td>4886</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1064245488</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99979</th>\n",
       "      <td>671</td>\n",
       "      <td>4896</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1065111996</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99980</th>\n",
       "      <td>671</td>\n",
       "      <td>4963</td>\n",
       "      <td>4.5</td>\n",
       "      <td>1065111855</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99981</th>\n",
       "      <td>671</td>\n",
       "      <td>4973</td>\n",
       "      <td>4.5</td>\n",
       "      <td>1064245471</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99982</th>\n",
       "      <td>671</td>\n",
       "      <td>4993</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1064245483</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99983</th>\n",
       "      <td>671</td>\n",
       "      <td>4995</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1064891537</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99984</th>\n",
       "      <td>671</td>\n",
       "      <td>5010</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1066793004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99985</th>\n",
       "      <td>671</td>\n",
       "      <td>5218</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1065111990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99986</th>\n",
       "      <td>671</td>\n",
       "      <td>5299</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1065112004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99987</th>\n",
       "      <td>671</td>\n",
       "      <td>5349</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1065111863</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99988</th>\n",
       "      <td>671</td>\n",
       "      <td>5377</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1064245557</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99989</th>\n",
       "      <td>671</td>\n",
       "      <td>5445</td>\n",
       "      <td>4.5</td>\n",
       "      <td>1064891627</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99990</th>\n",
       "      <td>671</td>\n",
       "      <td>5464</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1064891549</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99991</th>\n",
       "      <td>671</td>\n",
       "      <td>5669</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1063502711</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99992</th>\n",
       "      <td>671</td>\n",
       "      <td>5816</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1065111963</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99993</th>\n",
       "      <td>671</td>\n",
       "      <td>5902</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1064245507</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99994</th>\n",
       "      <td>671</td>\n",
       "      <td>5952</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1063502716</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99995</th>\n",
       "      <td>671</td>\n",
       "      <td>5989</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1064890625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99996</th>\n",
       "      <td>671</td>\n",
       "      <td>5991</td>\n",
       "      <td>4.5</td>\n",
       "      <td>1064245387</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99997</th>\n",
       "      <td>671</td>\n",
       "      <td>5995</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1066793014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99998</th>\n",
       "      <td>671</td>\n",
       "      <td>6212</td>\n",
       "      <td>2.5</td>\n",
       "      <td>1065149436</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99999</th>\n",
       "      <td>671</td>\n",
       "      <td>6268</td>\n",
       "      <td>2.5</td>\n",
       "      <td>1065579370</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100000</th>\n",
       "      <td>671</td>\n",
       "      <td>6269</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1065149201</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100001</th>\n",
       "      <td>671</td>\n",
       "      <td>6365</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1070940363</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100002</th>\n",
       "      <td>671</td>\n",
       "      <td>6385</td>\n",
       "      <td>2.5</td>\n",
       "      <td>1070979663</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100003</th>\n",
       "      <td>671</td>\n",
       "      <td>6565</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1074784724</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100004 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        userId  movieId  rating   timestamp\n",
       "0            1       31     2.5  1260759144\n",
       "1            1     1029     3.0  1260759179\n",
       "2            1     1061     3.0  1260759182\n",
       "3            1     1129     2.0  1260759185\n",
       "4            1     1172     4.0  1260759205\n",
       "5            1     1263     2.0  1260759151\n",
       "6            1     1287     2.0  1260759187\n",
       "7            1     1293     2.0  1260759148\n",
       "8            1     1339     3.5  1260759125\n",
       "9            1     1343     2.0  1260759131\n",
       "10           1     1371     2.5  1260759135\n",
       "11           1     1405     1.0  1260759203\n",
       "12           1     1953     4.0  1260759191\n",
       "13           1     2105     4.0  1260759139\n",
       "14           1     2150     3.0  1260759194\n",
       "15           1     2193     2.0  1260759198\n",
       "16           1     2294     2.0  1260759108\n",
       "17           1     2455     2.5  1260759113\n",
       "18           1     2968     1.0  1260759200\n",
       "19           1     3671     3.0  1260759117\n",
       "20           2       10     4.0   835355493\n",
       "21           2       17     5.0   835355681\n",
       "22           2       39     5.0   835355604\n",
       "23           2       47     4.0   835355552\n",
       "24           2       50     4.0   835355586\n",
       "25           2       52     3.0   835356031\n",
       "26           2       62     3.0   835355749\n",
       "27           2      110     4.0   835355532\n",
       "28           2      144     3.0   835356016\n",
       "29           2      150     5.0   835355395\n",
       "...        ...      ...     ...         ...\n",
       "99974      671     4034     4.5  1064245493\n",
       "99975      671     4306     5.0  1064245548\n",
       "99976      671     4308     3.5  1065111985\n",
       "99977      671     4880     4.0  1065111973\n",
       "99978      671     4886     5.0  1064245488\n",
       "99979      671     4896     5.0  1065111996\n",
       "99980      671     4963     4.5  1065111855\n",
       "99981      671     4973     4.5  1064245471\n",
       "99982      671     4993     5.0  1064245483\n",
       "99983      671     4995     4.0  1064891537\n",
       "99984      671     5010     2.0  1066793004\n",
       "99985      671     5218     2.0  1065111990\n",
       "99986      671     5299     3.0  1065112004\n",
       "99987      671     5349     4.0  1065111863\n",
       "99988      671     5377     4.0  1064245557\n",
       "99989      671     5445     4.5  1064891627\n",
       "99990      671     5464     3.0  1064891549\n",
       "99991      671     5669     4.0  1063502711\n",
       "99992      671     5816     4.0  1065111963\n",
       "99993      671     5902     3.5  1064245507\n",
       "99994      671     5952     5.0  1063502716\n",
       "99995      671     5989     4.0  1064890625\n",
       "99996      671     5991     4.5  1064245387\n",
       "99997      671     5995     4.0  1066793014\n",
       "99998      671     6212     2.5  1065149436\n",
       "99999      671     6268     2.5  1065579370\n",
       "100000     671     6269     4.0  1065149201\n",
       "100001     671     6365     4.0  1070940363\n",
       "100002     671     6385     2.5  1070979663\n",
       "100003     671     6565     3.5  1074784724\n",
       "\n",
       "[100004 rows x 4 columns]"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_MovieLens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "movieWatchedCount = {}\n",
    "userWatchCount = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "for m in df_MovieLens.movieId.unique():\n",
    "    movieWatchedCount[m] = len(df_MovieLens[df_MovieLens.movieId == m])\n",
    "for u in df_MovieLens.userId.unique():\n",
    "    userWatchCount[u] = len(df_MovieLens[df_MovieLens.userId == u])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_movieWatchedCount = sorted(movieWatchedCount.items(), key=operator.itemgetter(1), reverse=True)\n",
    "sorted_userWatchCount = sorted(userWatchCount.items(), key=operator.itemgetter(1), reverse=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Veri Matrisinin Oluşturulması\n",
    "\n",
    "Yoğun veya seyrek matris oluşturma isteğinize göre alttaki hücrelerden sadece birini çalıştırın. <br>\n",
    "Oluşturduğumuz matrisin satırları filmleri, sütunları kullanıcıları, elemanları ise kullanıcıların filmlere vermiş olduğu puanlamaları temsil etmektedir."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Yoğun Matrisin Üretilmesi\n",
    "\n",
    "Yoğun veri seti için MovieLens’ten en çok film izlemiş 20 kullanıcı ve en çok izlenen 50 filmi kullandık."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO\n",
    "#yoğun ve seyrek matrisi fonksiyonla çağır"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of Rows\n",
    "# nu: 1 -> W\n",
    "movieCount = 50\n",
    "\n",
    "# Number of Columns\n",
    "# tao: 1 -> K\n",
    "userCount = 20\n",
    "\n",
    "topMovies = np.asarray(sorted_movieWatchedCount)[:movieCount,0]\n",
    "topUsers = np.asarray(sorted_userWatchCount)[:userCount,0]\n",
    "\n",
    "userMovie = np.zeros((movieCount, userCount))\n",
    "\n",
    "for i, m in enumerate(topMovies):\n",
    "    for j, u in enumerate(topUsers):\n",
    "        if len(df_MovieLens[(df_MovieLens.userId  == u) & (df_MovieLens.movieId == m)]) != 0:\n",
    "            userMovie[i][j] = 2*float(df_MovieLens[(df_MovieLens.userId  == u) & (df_MovieLens.movieId == m)]['rating'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Seyrek Matrisin Üretilmesi\n",
    "\n",
    "Seyrek veri için ise her seferinde rastgele en çok izlenen 650 filmden 50, en çok film izlemiş 260 kullanıcıdan 20 tanesini kullandık."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of Rows\n",
    "# nu: 1 -> W\n",
    "movieCount = 50\n",
    "\n",
    "# Number of Columns\n",
    "# tao: 1 -> K\n",
    "userCount = 20\n",
    "\n",
    "sparsityParameter = 2\n",
    "\n",
    "movieIndex = np.random.permutation(movieCount*sparsityParameter)[:movieCount]\n",
    "userIndex = np.random.permutation(userCount*sparsityParameter)[:userCount]\n",
    "\n",
    "topMovies = []#id's\n",
    "topUsers = []#id's\n",
    "\n",
    "#find ids\n",
    "for mI in movieIndex:\n",
    "    topMovies.append(sorted_movieWatchedCount[mI][0])\n",
    "for uI in userIndex:\n",
    "    topUsers.append(sorted_userWatchCount[uI][0])\n",
    "\n",
    "\n",
    "topMovies = np.asarray(topMovies)\n",
    "topUsers = np.asarray(topUsers)\n",
    "\n",
    "userMovie = np.zeros((movieCount, userCount))\n",
    "\n",
    "for i, m in enumerate(topMovies):\n",
    "    for j, u in enumerate(topUsers):\n",
    "        if len(df_MovieLens[(df_MovieLens.userId  == u) & (df_MovieLens.movieId == m)]) != 0:\n",
    "            userMovie[i][j] = 2*float(df_MovieLens[(df_MovieLens.userId  == u) & (df_MovieLens.movieId == m)]['rating'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Maskeleme\n",
    "\n",
    "Eksik veriyi yani $x_{\\nu, \\tau}$ gözlemlenmemiş değerlerini modellerken *X* matrisi ile aynı boyuta sahip olan $\\textit{M} = \\left \\{ m_{\\nu, \\tau} \\right \\}$  *maske matrisi* aşağıdaki gibi tanımlanır:\n",
    "\n",
    "<center>\n",
    "$m_{\\nu, \\tau} = \\left\\{\\begin{matrix}\n",
    " 0,& x_{\\nu, \\tau} \\text{ gözlemlenmemişse},\\\\ \n",
    " 1,& x_{\\nu, \\tau} \\text{ gözlemlenmişse}.\n",
    "\\end{matrix}\\right.$\n",
    "</center>\n",
    "\n",
    "Bu maskeyi kullanarak olabilirlik fonksiyonunu şu şekilde yazılır:\n",
    "\n",
    "\\begin{align*}\n",
    "p(X,S\\mid T,V) = \\prod_{\\nu, \\tau}\\left ( p\\left (x_{\\nu, \\tau}\\mid s_{\\nu, 1:I, \\tau}\\right ) p\\left (s_{\\nu, 1:I, \\tau}\\mid t_{\\nu, 1:I}, v_{1:I, \\tau}  \\right )\\right )^{m_{\\nu, \\tau}}\n",
    "\\end{align*}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Maskeleme Metodları\n",
    "\n",
    "Bu çalışmamızda veriye *kısmi* ve *tam* olarak iki farklı açıdan yaklaşarak KL ıraksayı ve KOKH metrikleri (ileride açıklanacak) ile karşılaştırmalar yaptık. \n",
    "\n",
    "Kısmi yaklaşımda verimizi %30 test, %69 maskelenmiş, %1 başlangıçta bilinen olarak ayırdık. Maskelenmiş veriyi açtıkça değişen test üzerindeki hatayı ölçtük.\n",
    "\n",
    "Tam yaklaşımda ise %99 maskelenmiş, %1 başlangıçta bilinen olarak ayırdık ve hatayı bütün veri üzerinden ölçtük."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "def randomMasking(W, K, dataExistance):\n",
    "    dataIndices = np.argwhere(dataExistance>0)\n",
    "    \n",
    "    #test, mask, known\n",
    "    mask = np.zeros([W,K])\n",
    "    test = np.copy(dataExistance)\n",
    "    \n",
    "    np.random.shuffle(dataIndices)\n",
    "    \n",
    "    for i in range(30*len(dataIndices)//100):\n",
    "        test[dataIndices[i][0], dataIndices[i][1]] = 0\n",
    "    for i in range(30*len(dataIndices)//100, 31*len(dataIndices)//100):\n",
    "        mask[dataIndices[i][0], dataIndices[i][1]] = 1\n",
    "        \n",
    "    return mask, test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GÖZLEM SIRASI SEÇME STRATEJİLERİ\n",
    "\n",
    "Bu çalışmada, maskelenmiş olan veriyi gözlemlemek için beş farklı gözlem sırası seçme stratejisi tanımladık.\n",
    "\n",
    "Amacımız, en hızlı şekilde bütün veriyi ögrenebileceğimiz (yakınsayabilecegimiz) bilgi elde etme stratejisini bulmak ve ögrenme sonucunda test verisi hakkında doğru tahminlemede bulunmak."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Rastgele Strateji\n",
    "\n",
    "**Tanım**: Rastgele bir pozisyonda yer alan maskelenmiş veri gözlemlenir. <br>\n",
    "Gözlemlenmiş verideki bilgiyi kullanmadan maskelenmiş veriyi açar. Tanımlanan diğer stratejilerin işlevselliğini değerlendirmek için bir alt sınır oluşturur."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "def openRandom(W, K, mask, test):\n",
    "    openOrder = np.arange(W*K)\n",
    "    random.shuffle(openOrder)\n",
    "    \n",
    "    for i in openOrder:\n",
    "        if mask[i//K][i%K] == 0 and test[i//K][i%K] == 1:\n",
    "            return i//K, i%K\n",
    "        \n",
    "    return -1, -1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Satır-Sütun Stratejileri\n",
    "\n",
    "**Tanım**: Satır ve sütunlar en az veri gözlemlenenden en çok veri gözlemlenene dogru sıralanır. Satıra veya sütuna öncelik vererek kesişimlerindeki ilk maskelenmiş veri açılır. <br>\n",
    "Bir veriyi tahmin ederken o veri hakkında en çok bilgi edinebilecegimiz yerler o verinin bulunduğu satır ve sütunundaki diger verilerdir. Düzenli olarak satır ve sütunlardaki gözlemlenmiş veri sayısını artırarak bütün veri hakkındaki tahminimizi iyileştirebiliriz."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "def openMaxColMask(W, K, mask, test):\n",
    "    colSum = mask.sum(0)\n",
    "    rowSum = mask.sum(1)\n",
    "    \n",
    "    colMins = colSum.argsort()\n",
    "    rowMins = rowSum.argsort()\n",
    "\n",
    "    for c in colMins:\n",
    "        for r in rowMins:\n",
    "            if mask[r][c] == 0 and test[r][c] == 1:\n",
    "                return r, c\n",
    "            \n",
    "    \n",
    "    return openRandom(W, K, mask, test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "def openMaxRowMask(W, K, mask, test):\n",
    "    colSum = mask.sum(0)\n",
    "    rowSum = mask.sum(1)\n",
    "    \n",
    "    colMins = colSum.argsort()\n",
    "    rowMins = rowSum.argsort()\n",
    "\n",
    "    for r in rowMins:\n",
    "        for c in colMins:\n",
    "            if mask[r][c] == 0 and test[r][c] == 1:\n",
    "                return r, c\n",
    "            \n",
    "    return openRandom(W, K, mask, test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Maskelenmiş Verilerin Varyansı Stratejileri\n",
    "\n",
    "**Tanım**: Varyansın en küçük veya en büyük oldugu pozisyondaki veri gözlemlenir.<br>\n",
    "Varyasyonel Bayes sonucunda *T* ve *V* matrislerinin yanında bunları oluşturan *Gamma* dağılımı parametreleri de elde ediliyor. Bu parametreleri kullanarak belirlenen sayıda T ve V örnekleyerek tahminler üretilir. Bu tahminlerle, maskelenmiş olan kısımdaki her bir veri tahmininin varyansı hesaplanır. Bir pozisyondaki varyansın büyük olması, o pozisyon için üretilen tahmin değerinin belirsizliğinin fazla oldugunu göstermektedir. Küçük varyans ise belirsizliğin az olduğu yerdir."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "def openMinVar(W, K, mask, test, xCandidates):\n",
    "    var_candidate = np.var(xCandidates, axis=0)\n",
    "    ind = var_candidate.flatten().argsort()\n",
    "    \n",
    "    for i in ind:\n",
    "        if mask[i//K][i%K] == 0 and test[i//K][i%K] == 1:\n",
    "            return i//K, i%K\n",
    "        \n",
    "    return -1, -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "def openMaxVar(W, K, mask, test, xCandidates):\n",
    "    var_candidate = np.var(xCandidates, axis=0)\n",
    "    \n",
    "    ind = var_candidate.flatten().argsort()[::-1]\n",
    "    \n",
    "    for i in ind:\n",
    "        if mask[i//K][i%K] == 0 and test[i//K][i%K] == 1:\n",
    "            return i//K, i%K\n",
    "    \n",
    "    return -1, -1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gözlemlenmiş Satır Varyans Stratejisi\n",
    "\n",
    "**Tanım**: Her satır için o satırdaki açılmış olan veriler üzerinden o satırın varyansı hesaplanır. Satırlar bu varyans hesabına göre büyükten küçüge sıralanır. Sütunlar en az veri gözlemlenenden en çok veri gözlemlenene dogru sıralanır. Satıra öncelik vererek sütunlarla olan kesişimindeki ilk maskelenmiş veri açılır.<br>\n",
    "Veri setimizde satırlar filmlere karşılık gelmektedir. Bu stratejimizde her filme verilen puanın varyansı hesaplanır. Genellikle bir filme benzer puanlar verilmesi beklenir. Burada,\n",
    "varyansı büyük olan filmlerin puan aralığı daha geniştir. Bu filmlere verilen puanların tahmin edilmesi daha zordur."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "def openRowVarColMask(W, K, mask, test, xCandidate):\n",
    "    mean_candidate = mask*xCandidate\n",
    "    rowIndVarSorted = np.argsort(np.nan_to_num(np.nanvar(np.where(mean_candidate!=0,mean_candidate,np.nan), axis=1)))[::-1]\n",
    "    \n",
    "    colSum = mask.sum(0)\n",
    "    colMins = colSum.argsort()\n",
    "    \n",
    "    for r in rowIndVarSorted:\n",
    "        for c in colMins:\n",
    "            if mask[r][c] == 0 and test[r][c] == 1:\n",
    "                return r, c\n",
    "         \n",
    "    return openRandom(W, K, mask, test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Popularity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "def openRandomPopularity(W, K, mask, test, dataExistance):\n",
    "    counts = np.sum(dataExistance, axis=1)\n",
    "    counts = np.argsort(counts)[::-1]\n",
    "    selections = []\n",
    "    movieCount=0\n",
    "    for movie in counts:\n",
    "        movieCount+=1\n",
    "        for user in set(np.where(mask[movie]==0)[0])&set(np.where(test[movie]==1)[0]):\n",
    "            selections.append((movie, user))\n",
    "        if len(selections)>=20 and movieCount>=W//20:\n",
    "            break\n",
    "    \n",
    "    random.shuffle(selections)\n",
    "    \n",
    "    try:\n",
    "        return selections[0][0], selections[0][1]\n",
    "    except:\n",
    "        return openRandom(W, K, mask, test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Popularity Variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "def openPopularityVariance(W, K, mask, test, dataExistance, xCandidate):\n",
    "    popularity = np.sum(dataExistance, axis=1)\n",
    "    \n",
    "    mean_candidate = mask*xCandidate\n",
    "    rowIndVar = np.nan_to_num(np.nanvar(np.where(mean_candidate!=0,mean_candidate,np.nan), axis=1))\n",
    "\n",
    "    counts = np.log(popularity)*rowIndVar\n",
    "    counts = np.argsort(counts)[::-1]\n",
    "\n",
    "    for movie in counts:\n",
    "        for user in set(np.where(mask[movie]==0)[0])&set(np.where(test[movie]==1)[0]):\n",
    "            return movie, user\n",
    "    \n",
    "    return openRandom(W, K, mask, test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Başlatma"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model seçimi\n",
    "\n",
    "Önerdigimiz yöntemi MovieLens verisi üzerinde deniyoruz. Burada $W = 50$ (satır/film sayısı), $K = 20$ (sütun/kullanıcı sayısı) ve kaynakların sayısı $I = 4$ olarak belirledik. Gerçek modelin hiperparametrelerini ise $a^{t} = b^{t} = 1$ ve $a^{v} = b^{v} = 1$ olarak aldık."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of Rows\n",
    "# nu: 1 -> W\n",
    "W = movieCount;\n",
    "\n",
    "# Number of Columns\n",
    "# tao: 1 -> K\n",
    "K = userCount\n",
    "\n",
    "# Number of templates\n",
    "I = 4;\n",
    "\n",
    "# Set prior parameters \n",
    "A_t = np.ones([W,I]) # Shape\n",
    "B_t = np.ones([W,I]) # Scale\n",
    "A_v = np.ones([I,K])\n",
    "B_v = np.ones([I,K])\n",
    "\n",
    "# Generate a random template and excitation\n",
    "L_t = E_t = np.random.gamma(A_t,np.divide(B_t, A_t))\n",
    "L_v = E_v = np.random.gamma(A_v,np.divide(B_v, A_v))\n",
    "\n",
    "x = userMovie.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "#strategyLabels = [\"Random\", \"Row Mask Eq\", \"Min Var\", \"Max Var\", \"Row Var Col Mask\"]\n",
    "strategyLabels = [\"Rastgele\", \"Satır-Sütun\", \"Min Varyans\", \"Maks Varyans\", \"Satır Varyans\", \"Random Popularity\", \"Popularity Variance\"]\n",
    "strategyColors = [\"b\",\"r\",\"y\",\"k\",\"m\",\"c\", \"g\"]\n",
    "\n",
    "#errorMetricLabels = [\"RMSE_Partial\", \"RMSE_Full\", \"KL_Partial\", \"KL_Full\"]\n",
    "errorMetricLabels = [\"KOKH Kısmi\", \"KOKH Tam\", \"KL Iraksayı Kısmi\", \"KL Iraksayı Tam\"]\n",
    "\n",
    "# dataExistance: True if data exist, False othwerwise\n",
    "# mask: True if mask opened, False if masked\n",
    "# test: True if not test data and data  exist, False if test data or no data exist\n",
    "#       For testing we use dataExistance and test together\n",
    "#       For cell opening we use test with mask\n",
    "dataExistance = userMovie > 0\n",
    "mask, test = randomMasking(W,K,dataExistance)\n",
    "\n",
    "allLikelihood = []\n",
    "allOpenedCells = []\n",
    "\n",
    "allDiffRMSEPartial = []\n",
    "allDiffRMSEFull = []\n",
    "allDiffKLPartial = []\n",
    "allDiffKLFull = []\n",
    "\n",
    "allErrorDiffs = []\n",
    "\n",
    "allRMSEPartial = []\n",
    "allRMSEFull = []\n",
    "allKLPartial = []\n",
    "allKLFull = []\n",
    "\n",
    "allError = []\n",
    "\n",
    "allEstimationVariance = []\n",
    "\n",
    "allStrategyLabels = []\n",
    "allStrategyColors = []\n",
    "\n",
    "KLsampleSize = 1000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Olabilirlik hesabı"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculateLikelihood(x, xPredicted, test, dataExistance, W, K):\n",
    "    lh = 0\n",
    "    for w in range(W):\n",
    "        for k in range(K):\n",
    "            lh += (dataExistance[w,k]^test[w,k]) * (x[w,k] * np.log(xPredicted[w,k]) - xPredicted[w,k] - special.gammaln(x[w,k]+1))\n",
    "    return lh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Adayların örneklenmesi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sampleCandidates(a_t, b_t, a_v, b_v, sampleSize=100):\n",
    "    xCandidates = []\n",
    "    for i in range(sampleSize):\n",
    "        T = np.random.gamma(a_t,b_t)\n",
    "        V = np.random.gamma(a_v,b_v)\n",
    "\n",
    "        xCandidates.append(np.dot(T, V))\n",
    "        \n",
    "    return np.asarray(xCandidates)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Varyansın hesaplanması ve örneklenen X adaylarından gelen hata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Değerlendirme Metrikleri\n",
    "\n",
    "Yaklaşımımızın performansını değerlendirmek için çeşitli yöntemler bulunmaktadır. Bu çalışmamızda, popüler olan Kullback-Leibler (KL) ıraksayı ve Kök Ortalama Kare Hata (KOKH) olmak üzere iki ayrı metrik kullandık. Bu metrikler sırasıyla aşağıdaki gibi tanımlanır:\n",
    "\n",
    "\\begin{align*}\n",
    "D_{KL}\\left ( X \\parallel \\widehat{X} \\right ) = \\sum_{i,j} X\\left ( i,j \\right ) \\log \\frac{X\\left ( i,j \\right )}{\\widehat{X}\\left ( i,j \\right )}\n",
    "\\end{align*}\n",
    "\n",
    "\\begin{align*}\n",
    "KOKH = \\sqrt{\\frac{1}{X_{test}}\\sum_{i,j}\\left ( X(i,j) -  \\widehat{X}(i,j)\\right )^{2}}\n",
    "\\end{align*}\n",
    "Burada, $\\widehat{X}(i,j)$ önerilen metod tarafından *i* filmi hakkında *j* kullanıcısının verdiği tahmin edilen oylama değerini ve $X_{test}$ ise, toplam test edilen oylama sayısını göstermektedir."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculateVarianceAndError(x, test, dataExistance, xCandidates):\n",
    "    mean_candidate = np.mean(xCandidates, axis=0)\n",
    "    varEst = np.var(xCandidates, axis=0)\n",
    "    varEst = 1.0 * (varEst - varEst.min()) / (varEst.max() - varEst.min())\n",
    "\n",
    "    diffMeanEst = abs((dataExistance^test)*mean_candidate - (dataExistance^test)*x)\n",
    "    return varEst, diffMeanEst"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Adayların dağılıma dönüştürülmesi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transformCandidatesToDistributions(candidates, sampleSize, test, dataExistance):\n",
    "    candidates = np.round(candidates)\n",
    "    candidates = np.minimum(candidates,10*np.ones(candidates.shape)).astype(int)\n",
    "    candidates = np.maximum(candidates,np.ones(candidates.shape)).astype(int)\n",
    "    candidates = (dataExistance^test) * candidates\n",
    "    \n",
    "    candidateDistributions = []\n",
    "    \n",
    "    for i in range(W):\n",
    "        for j in range(K):\n",
    "            if candidates[0,i,j] != 0:\n",
    "                y = np.bincount(candidates[:,i,j])\n",
    "                c = np.zeros(11)\n",
    "                c[:len(y)] += y\n",
    "                c = np.maximum(c,0.00000001*np.ones(c.shape))\n",
    "                c /= sampleSize\n",
    "                candidateDistributions.append(c[1:])\n",
    "            else:\n",
    "                candidateDistributions.append(np.ones(10))\n",
    "                \n",
    "    return candidateDistributions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### KL ıraksayı hesabı"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculateKLdivergence(xCandidates, bestCandidateDistributions, sampleSize, test, dataExistance):\n",
    "    xCandidateDistributions = transformCandidatesToDistributions(xCandidates, sampleSize, test, dataExistance)\n",
    "    return entropy(np.asarray(xCandidateDistributions).T, np.asarray(bestCandidateDistributions).T).reshape((W,K))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "def variatonalBayes(X, L_t, L_v, E_t, E_v, mask, MAXITER, likelihood = None, test = None, dataExistance = None):\n",
    "    W = L_t.shape[0]\n",
    "    K = L_v.shape[1]\n",
    "    I = L_t.shape[1]\n",
    "    \n",
    "    \n",
    "    \n",
    "    for n in range(MAXITER):\n",
    "        prev_t = E_t.copy()\n",
    "        prev_v = E_v.copy()\n",
    "        masked_X = np.multiply(X, mask)\n",
    "        \n",
    "        sigma_t = np.multiply(L_t, np.matmul(np.divide(masked_X, np.matmul(L_t,L_v)),L_v.T))\n",
    "        sigma_v = np.multiply(L_v, np.matmul(L_t.T, np.divide(masked_X, np.matmul(L_t, L_v))))\n",
    "        \n",
    "        a_t = A_t + sigma_t\n",
    "        a_v = A_v + sigma_v\n",
    "        \n",
    "        b_t = 1/( np.divide(A_t, B_t) + np.dot(mask, np.transpose(E_v)) )\n",
    "        E_t = np.multiply(a_t, b_t)\n",
    "        \n",
    "        b_v = 1/( np.divide(A_v, B_v) + np.dot(np.transpose(E_t), mask))\n",
    "        E_v = np.multiply(a_v, b_v)\n",
    "        \n",
    "        L_t = np.multiply(np.exp(digamma(a_t)), b_t)\n",
    "        L_v = np.multiply(np.exp(digamma(a_v)), b_v)\n",
    "        \n",
    "        if likelihood != None:\n",
    "            likelihood.append(calculateLikelihood(X, np.matmul(L_t, L_v), test, dataExistance, W, K))\n",
    "        \n",
    "    if likelihood == None:\n",
    "        return L_t, L_v, a_t, b_t, a_v, b_v\n",
    "    else:\n",
    "        return L_t, L_v, a_t, b_t, a_v, b_v, likelihood"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## En iyi ayrıştırmanın hesaplanması"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "t0 = time.time()\n",
    "\n",
    "T = E_t.copy()\n",
    "V = E_v.copy()\n",
    "MAXITER = 10000\n",
    "\n",
    "T, V, a_t, b_t, a_v, b_v = variatonalBayes(x, L_t, L_v, T, V, dataExistance.copy(), MAXITER)\n",
    "bestCandidates = sampleCandidates(a_t, b_t, a_v, b_v, sampleSize=1000)\n",
    "bestCandidatesMean = np.mean(bestCandidates, axis=0)\n",
    "bestCandidateDistributionsFull = transformCandidatesToDistributions(bestCandidates, KLsampleSize, test&False, dataExistance|True)\n",
    "bestCandidateDistributionsPartial = transformCandidatesToDistributions(bestCandidates, KLsampleSize, test, dataExistance)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### İlk-ısınma (Burn-in) periyodu\n",
    "\n",
    "1000 adımlık bir ilk-ısınma devresi uyguladık."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.11866140365600586\n"
     ]
    }
   ],
   "source": [
    "t0 = time.time()\n",
    "T = E_t.copy()\n",
    "V = E_v.copy()\n",
    "\n",
    "T, V, _, _, _, _ = variatonalBayes(x, L_t, L_v, T, V, mask.copy(), MAXITER = 1000)\n",
    "\n",
    "modT = T.copy()\n",
    "modV = V.copy()\n",
    "print(time.time()-t0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "78\n"
     ]
    }
   ],
   "source": [
    "for cellOpenStrategy in range(7):\n",
    "    t0 = time.time()\n",
    "    \n",
    "    likelihood = []\n",
    "    openedCells = [0]\n",
    "    \n",
    "    diffErrorPartial = []\n",
    "    diffErrorFull = []\n",
    "    diffKLPartial = []\n",
    "    diffKLFull = []\n",
    "    varEst = []\n",
    "\n",
    "    T = modT.copy()\n",
    "    V = modV.copy()\n",
    "\n",
    "    maskX = mask.copy()\n",
    "\n",
    "    cellOpenCount = 10\n",
    "    extraIter = 20\n",
    "    EPOCH = int((dataExistance.sum()-(dataExistance^test).sum()-mask.sum())//cellOpenCount + extraIter)\n",
    "    print(EPOCH)\n",
    "    MAXITER = 20\n",
    "\n",
    "    for nn in range(EPOCH):\n",
    "        if nn >= (dataExistance.sum()-(dataExistance^test).sum()-mask.sum())//cellOpenCount:\n",
    "            MAXITER = 50\n",
    "        T, V, a_t, b_t, a_v, b_v, likelihood = variatonalBayes(x, L_t, L_v, T, V, maskX, MAXITER, likelihood, test, dataExistance)\n",
    "\n",
    "        # Take Mean estimate and calculate diff from X\n",
    "        xCandidates = sampleCandidates(a_t, b_t, a_v, b_v, sampleSize=100)\n",
    "\n",
    "        ve, de = calculateVarianceAndError(bestCandidatesMean, test, dataExistance, xCandidates)\n",
    "        varEst.append(ve)\n",
    "        diffErrorPartial.append(de)\n",
    "        \n",
    "        _, de = calculateVarianceAndError(bestCandidatesMean, test&False, dataExistance|True, xCandidates)\n",
    "        diffErrorFull.append(de)\n",
    "        \n",
    "        de = calculateKLdivergence(xCandidates, bestCandidateDistributionsPartial, KLsampleSize, test, dataExistance)\n",
    "        diffKLPartial.append(de)\n",
    "        \n",
    "        de = calculateKLdivergence(xCandidates, bestCandidateDistributionsFull, KLsampleSize, test&False, dataExistance|True)\n",
    "        diffKLFull.append(de)\n",
    "\n",
    "        # Apply Cell Opening Strategy\n",
    "        for co in range(cellOpenCount):\n",
    "            if cellOpenStrategy == 0:\n",
    "                row, col = openRandom(W, K, maskX, test)\n",
    "            elif cellOpenStrategy == 1:\n",
    "                if nn % 2 == 0:\n",
    "                    row, col = openMaxColMask(W, K, maskX, test)\n",
    "                else:\n",
    "                    row, col = openMaxRowMask(W, K, maskX, test)\n",
    "            elif cellOpenStrategy == 2:\n",
    "                row, col = openMinVar(W, K, maskX, test, xCandidates)\n",
    "            elif cellOpenStrategy == 3:\n",
    "                row, col = openMaxVar(W, K, maskX, test, xCandidates)\n",
    "            elif cellOpenStrategy == 4:\n",
    "                row, col = openRowVarColMask(W, K, maskX, test, np.dot(T,V))\n",
    "            elif cellOpenStrategy == 5:\n",
    "                row, col = openRandomPopularity(W, K, maskX, test, dataExistance)\n",
    "            elif cellOpenStrategy == 6:\n",
    "                row, col = openPopularityVariance(W, K, maskX, test, dataExistance, np.dot(T,V))\n",
    "            else:\n",
    "                row, col = (-1, -1)\n",
    "\n",
    "\n",
    "            # Remove mask from (row, col)\n",
    "            if not (row == -1 and col == -1):\n",
    "                maskX[row][col] = 1\n",
    "            openedCells.append((row,col))\n",
    "\n",
    "\n",
    "    allStrategyLabels.append(cellOpenStrategy)\n",
    "    allStrategyColors.append(cellOpenStrategy)\n",
    "\n",
    "    allLikelihood.append(likelihood)\n",
    "    allOpenedCells.append(openedCells)\n",
    "\n",
    "    allEstimationVariance.append(varEst)\n",
    "    \n",
    "\n",
    "    allDiffRMSEPartial.append(diffErrorPartial)\n",
    "    allDiffRMSEFull.append(diffErrorFull)\n",
    "    allDiffKLPartial.append(diffKLPartial)\n",
    "    allDiffKLFull.append(diffKLFull)\n",
    "    \n",
    "    \n",
    "    rmse = np.zeros(len(diffErrorPartial))\n",
    "    for i in range(len(diffErrorPartial)):\n",
    "        # RMSE\n",
    "        de2 = diffErrorPartial[i]*diffErrorPartial[i]\n",
    "        rmse[i] = np.sqrt(np.nanmean(np.where(de2!=0,de2,np.nan)))\n",
    "    allRMSEPartial.append(rmse)\n",
    "    \n",
    "    rmse = np.zeros(len(diffErrorFull))\n",
    "    for i in range(len(diffErrorFull)):\n",
    "        # RMSE\n",
    "        de2 = diffErrorFull[i]*diffErrorFull[i]\n",
    "        rmse[i] = np.sqrt(np.nanmean(np.where(de2!=0,de2,np.nan)))\n",
    "    allRMSEFull.append(rmse)\n",
    "    \n",
    "    kl = np.zeros(len(diffKLPartial))\n",
    "    for i in range(len(diffKLPartial)):\n",
    "        kl[i] = np.mean(diffKLPartial[i])\n",
    "    allKLPartial.append(kl)\n",
    "    \n",
    "    kl = np.zeros(len(diffKLFull))\n",
    "    for i in range(len(diffKLFull)):\n",
    "        kl[i] = np.mean(diffKLFull[i])\n",
    "    allKLFull.append(kl)\n",
    "    \n",
    "    \n",
    "    print(strategyLabels[cellOpenStrategy] + \" %0.3fs. de tamamlandı.\" % (time.time() - t0))\n",
    "    \n",
    "allErrorDiffs.append(allDiffRMSEPartial)\n",
    "allErrorDiffs.append(allDiffRMSEFull)\n",
    "allErrorDiffs.append(allDiffKLPartial)\n",
    "allErrorDiffs.append(allDiffKLFull)\n",
    "\n",
    "allError.append(allRMSEPartial)\n",
    "allError.append(allRMSEFull)\n",
    "allError.append(allKLPartial)\n",
    "allError.append(allKLFull)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Etkileşimli Hata ve Varyans Isı haritası\n",
    "\n",
    "Satır-sütun stratejisi ve KL ıraksayı hata metriği yoğun matris üzerinde kullanılmış bir deney süreci gösterilmektedir. Çıktıda hata ve varyans ısı haritasını görmekteyiz. Bu grafikte, kırmızı ile hata, mavi ile maskelenmiş verinin varyansı ve beyaz ile ise gözlemlenmiş veri gösterilmektedir. Kırmızı ve mavinin tonları hata ve varyansın şiddetini yansıtmaktadır. Yinelemeler süresince veriler gözlemlendikçe maskelenmiş (mavilikler) verinin kaybolduğunu (beyaza dönmesini), hatanın (kırmızılıkların) ise azaldığını grafikten görmekteyiz. Sağdaki grafikte KL ıraksayı kullanılarak elde edilen hata grafiğini ve ona oturtulan polinomu görmekteyiz."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cmap = matplotlib.colors.LinearSegmentedColormap.from_list('my_colormap',\n",
    "                                           ['blue','white', 'red'],\n",
    "                                           256) \n",
    "\n",
    "chosenStrategy = 6\n",
    "chosenErrorMetric = 2\n",
    "\n",
    "rmseHM = allError[chosenErrorMetric][chosenStrategy]\n",
    "matrixHM = allErrorDiffs[chosenErrorMetric][chosenStrategy]\n",
    "openedCellsHM = allOpenedCells[chosenStrategy]\n",
    "varEstHM = allEstimationVariance[chosenStrategy]\n",
    "\n",
    "xAxis = list(range(len(rmseHM)))\n",
    "xp = np.linspace(0, EPOCH-1, 1000)\n",
    "polDeg = 10\n",
    "p30 = np.poly1d(np.polyfit(xAxis, rmseHM, polDeg))\n",
    "\n",
    "vmax = rmseHM.max()\n",
    "    \n",
    "#matrixHM = diffErrorHM.copy()\n",
    "for i in range(EPOCH-extraIter):\n",
    "    #if openedCellsHM[i] == (-1,-1):\n",
    "    #    break\n",
    "    for oc in openedCellsHM[1+(i*cellOpenCount):]:\n",
    "        if oc[0] == -1:\n",
    "            break\n",
    "        matrixHM[i][oc[0],oc[1]] = -varEstHM[i][oc[0],oc[1]]*vmax\n",
    "    \n",
    "\n",
    "def pltErrorVarianceHM(rc):\n",
    "    fig = plt.figure(figsize=(8,8))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    if rc == 0:\n",
    "        plt.title(\"İlk Durum\")\n",
    "    elif rc <= (EPOCH-extraIter):\n",
    "        plt.title(\"Yineleme Sayısı: \" + str(rc))\n",
    "    else:\n",
    "        plt.title(\"Yineleme Sayısı: \" + str(rc))\n",
    "        \n",
    "    img = plt.imshow(matrixHM[rc],interpolation='nearest',\n",
    "                    cmap = cmap,\n",
    "                    origin='lower',\n",
    "                    vmax = vmax,\n",
    "                    vmin = -vmax)\n",
    "    plt.colorbar(img,cmap=cmap)\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot( range(rc+1), rmseHM[:rc+1])\n",
    "    \n",
    "    try:\n",
    "        xpRC = list(xp).index(xp[xp>=(rc+1)][0])\n",
    "    except:\n",
    "        xpRC = list(xp).index(xp[xp>=(rc)][0])\n",
    "    plt.plot( xp[:xpRC], p30(xp)[:xpRC], \"-\", color='r', linewidth=2)\n",
    "    \n",
    "    plt.xlim(0,EPOCH)\n",
    "    #plt.ylim(0,rmseHM.max()+1)\n",
    "    font = {'size'   : 15}\n",
    "    #plt.title(\"KL : \" + str(p30(xp)[xpRC-1]))\n",
    "    plt.xlabel(\"Yineleme Sayısı\", **font)\n",
    "    plt.ylabel(errorMetricLabels[chosenErrorMetric], **font)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "interact(pltErrorVarianceHM, rc = (0, 2*EPOCH-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0, 2*EPOCH, 5):\n",
    "    interact(pltErrorVarianceHM, rc = (0, i, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Yoğun ve seyrek veri üzerinde, tanımlanmış beş strateji için 10’lu açarak kısmi KL ıraksayı değişim grafiği\n",
    "\n",
    "Stratejilerin performanslarını, eşik değerine ne kadar hızlı ulaştıklarını karşılaştırarak ölçtük. Eşik değerine ulaşma metriği olarak *eğri altında kalan alanı* kullandık. En iyi strateji, eğri altında kalan alanı en az olandır.\n",
    "\n",
    "Hata fonksiyonlarının asıl davranışını gözlemleyebilmek için fonksiyonlara polinom oturttuk. Bu polinomların salınımdan daha az etkilenmesi için ise bütün veriler gözlemlendikten\n",
    "sonra 1000 yinelemeli Varyasyonel Bayes çalıştırdık."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xAxis = list(range(len(allKLPartial[0])))\n",
    "xp = np.linspace(0, EPOCH-1, 1000)\n",
    "polDeg = 15\n",
    "\n",
    "chosenErrorMetric = 2\n",
    "\n",
    "errorFunction = allError[chosenErrorMetric].copy()\n",
    "thr = 0\n",
    "for rmse in errorFunction:\n",
    "    thr += np.mean(rmse[-15:-5])\n",
    "thr /= len(errorFunction)\n",
    "\n",
    "\n",
    "fig = plt.figure(figsize=(10,10))\n",
    "\n",
    "plt.plot(range(EPOCH - extraIter+15), (EPOCH - extraIter+15)*[thr], \"--\", color='c', label=\"Eşik: \"+str(thr)[:5], linewidth=2)\n",
    "\n",
    "aucTrapz = []\n",
    "aucSimps = []\n",
    "\n",
    "\n",
    "xpRC = list(xp).index(xp[xp>=(EPOCH - extraIter+10)][0])\n",
    "\n",
    "for i, rmse in enumerate(errorFunction):\n",
    "    p30 = np.poly1d(np.polyfit(xAxis, rmse, polDeg))\n",
    "    aucTrapz.append(np.trapz(p30(xp)[:xpRC]-thr, x=xp[:xpRC]))\n",
    "    aucSimps.append(np.trapz(p30(xp)[:xpRC], x=xp[:xpRC]))\n",
    "    print(np.trapz(p30(xp)[:xpRC]-thr, x=xp[:xpRC]))\n",
    "    zz = i\n",
    "    if i == 1:\n",
    "        zz = 10\n",
    "    plt.plot( xp[:xpRC], p30(xp)[:xpRC], \"-\", label=strategyLabels[allStrategyLabels[i]], color=strategyColors[allStrategyColors[i]], linewidth=2, zorder=zz)\n",
    "\n",
    "    \n",
    "plt.xlim(0,)\n",
    "#plt.ylim(0,)\n",
    "\n",
    "font = {'size'   : 18}\n",
    "plt.xlabel(\"Yineleme Sayısı (\" + str(cellOpenCount) + \"'lu açma)\", **font)\n",
    "plt.ylabel(errorMetricLabels[chosenErrorMetric], **font)\n",
    "\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vargılar\n",
    "\n",
    "Bu çalışmamızda negatif olmayan matris ayrışımı için eşlenik Gamma önselleri ile hiyerarşik bir model inceledik ve çıkarımlar için Varyasyonel Bayes kullandık. Buradan yola çıkarak aktif eleman seçimi [7] problemine çözüm önerdik. Tanımladığımız beş stratejiyi KL ıraksayı ve KOKH metrikleri üzerinden karşılaştırdık. Yaptığımız deneyler ile satır-sütun stratejisinin etkili bir aktif ögrenme tekniği olduğunu gösterdik. Satır-sütun stratejisinin başarısının gözlemlenmemiş veriyi dengeli bir biçimde açıyor olmasına bağlıyoruz. Ayrıca bu stratejinin verinin içeriğinden bağımsız olarak tanımlanmış olması bu stratejiyi diğer alanlara da uygulanabilir kılıyor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kaynaklar\n",
    "\n",
    "[1] D. D. Lee and H. S. Seung, \"Learning the parts of objects\n",
    "with nonnegative matrix factorization.\", Nature, 401:788–791, 1999.\n",
    "\n",
    "[2]  A. T. Cemgil, \"Bayesian inference in non-negative matrix\n",
    "factorisation models.\", Technical Report CUED/FINFENG/TR.609, University of Cambridge, July 2008. Submitted for publication to Computational Intelligence and Neuroscience\n",
    "\n",
    "[3]  D. D. Lee and H. S. Seung, \"Algorithms for non-negative matrix factorization.\", Advances in neural information processing systems. 2001.\t\n",
    " \n",
    "[4] J. S. Liu, \"Monte Carlo Strategies in Scientific Computing\", Springer, New York, NY, USA, 2004.\n",
    " \n",
    "[5] W. R. Gilks, S. Richardson, and D. J. Spiegelhalter, Eds., \"Markov Chain Monte Carlo in Practice\", CRC Press, London, UK, 1996.\n",
    " \n",
    "[6] Harper, F. Maxwell, and Joseph A. Konstan, \"The movielens datasets: History and context.\", ACM Transactions on Interactive Intelligent Systems (TiiS) 5.4 (2016): 19.\n",
    " \n",
    "[7] Silva, Jorge, and Lawrence Carin. \"Active learning for online bayesian matrix factorization.\", Proceedings of the 18th ACM SIGKDD international conference on Knowledge discovery and data mining. ACM, 2012.\n",
    "\n",
    "[8] Suyunu, Burak, Gönül Ayci, and A. Taylan Cemgil. \"Active selection of elements for Bayesian nonnegative matrix factorization.\" 2018 26th Signal Processing and Communications Applications Conference (SIU). IEEE, 2018."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  },
  "widgets": {
   "state": {
    "aed19adf2fb24c898e7118f18c55b523": {
     "views": [
      {
       "cell_index": 47
      }
     ]
    }
   },
   "version": "1.2.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
